# -*- coding: utf-8 -*-
"""ELL_786_PART_C.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a_lcbVPffQt1_Pb9-0GFjuEpwfhlVvjP

## Giving N phrases as Input
"""

phrases = "Fettiplace, Elinor – Gerber, Daniel Frank – Guinness, Arthur – Hill, Georgiana – Hunziker, Otto Frederick – Lawson, Nigella – P. Halfhill, Albert – Pepper, Charles T. – Petty, Florence – Philippe, Claude – Roux, Michel – Rückerschöld, Anna Maria – Tremo, Paul – Women in brewing – Mario Peruzzi-Elizabeth David bibliography – The Art of Cookery Made Plain and Easy – A Book of Mediterranean Food – Compendium ferculorum, albo Zebranie potraw – The Compleat Housewife – The Cookery Book of Lady Clark of Tillypronie – The Experienced English Housekeeper  – Food in England – The Good Huswifes Jewell  – The Modern Cook – Modern Cookery for Private Families  – Mrs. Beeton's Book of Household Management  – A New System of Domestic Cookery – The Accomplisht Cook-Apple Maggot Quarantine Area – Arab Agricultural Revolution – Butler café – Cellarette – French fry vending machine – Makiyakinabe – Pancake machine – Wet market – 1950s American automobile culture – Acting white – Afternoon – Alcohol and Native Americans – Alcoholic beverages in Oregon – Algorithmic bias – Ali, Nimco – Anthony, Susan B. II – Anti-nuclear movement in Australia – Arizmendi Mejia, Elena – Astrology – Ave Imperator, morituri te salutant – Bailey, Frank (firefighter) – Banditti of the Prairie – Banglapedia – Bednar, David – Benet de Mewton, Milagros – Bic Cristal – Bikini – Body piercing – Boerehaat – Boobrie – Bosnian genocide denial – Breaker boy – Brownie Mary – Bugchasing – Cannabis in Japan – Cannabis in Oregon – Campbell, Bobbi – Campbell, John Gregorson – Child prostitution – Chongqing model – Clussexx Three D Grinchy Glee – Cologne Central Mosque – Compulsory Miseducation – Concealed shoes – Cone sisters – Conway, Samuel – Crinoline – Crypt of Civilization – Cultural depictions of spiders – Culture of the Cook Islands – Death in Singapore – Death of Jeffrey Epstein – Destruction of ivory – Disgusted of Tunbridge Wells – Dolphin drive hunting – Double burden – Dreamtime (book) – Dulębianka, Maria – Engineer boot – Entertainment – EST and The Forum in popular culture – Elixir (perfume) – Extremely online – Family structure in the United States – First World – Flitch of bacon custom – Foreign workers in Saudi Arabia – Freemasonry and women – Furry convention – Gay bathhouse – Genevieve Lhermitte – Giurchescu, Anca – Globalization and women in China – Goathouse Refuge – Hassan Al Kontar – Heat (perfume) – Heritage preservation in South Korea – Hernandez, Aileen – Ho Yuen Hoe – Homme by David Beckham – Hooray Henry – Hotel Europa – How to Blow Up a Pipeline – Intercrural sex – Itliong, Larry – Janet Jackson as a gay icon – Jayne Mansfield in popular culture – Jewellery of the Berber cultures – Joint attention – Kato, Sogen – Khanjar – Lang, Marie – Lesbian – Madzimbamuto, Stella – Manal al-Sharif – Manasollasa – Mari Lwyd – Meade, David – Metal corset – Metallic Metals Act – Mormon folklore – Mussie – Mutiny of the Matoika – Native American mascot controversy – New World Order (conspiracy theory) – Ni Yulan – Nonviolent Communication – Otaku – Overhill Cherokee – Pastel QAnon – Pi de les Tres Branques – Pizzagate conspiracy theory – Plastic Brit – Plumlee, Sybil – Polyethnicity – Project Chanology – The Psychology of The Simpsons – QAnon – Reborn doll – Redskin – Requiem for a Species – Rise (perfume) – S by Shakira – Same-sex marriage in Maryland – Sarria, José – Sarsour, Linda – Saviola, Marilyn – Sea in culture – Sexual abuse by yoga gurus – Sheng nu – Sikh diaspora – Sindy – Slavery in Haiti – Smith, Mary Stuart – Smoking in North Korea – Social heuristics – Soljak, Miriam – Sorensen, Jacki – Spirit Fruit Society – Squatting in Thailand – Squatting in the Czech Republic – Squatting in the Netherlands – Squatting in the Philippines  – Squatting in Ukraine – Stay-at-home dad – Steinem, Gloria – Still Jennifer Lopez – Tamimi, Ahed – Telpuk, María del Luján – Tewkesbury Medieval Festival – Think of the children – Three Sisters Tavern – Thunberg, Greta – Tinne, Emily – Tiny Town (miniature park) – Tree That Owns Itself – Tuberculosis in human culture – Venezuelan refugee crisis – Veterans benefits for post-traumatic stress disorder in the United States – Washington Redskins name controversy – Ways That Are Dark – Weapon dance – Whale tail – Whaling in the Faroe Islands – Woman's club movement in the United States – Wongso, Pah – Workhouse – Yousafzai, Malala – Zuby – Antankarana – Antemoro people – Armenian Americans  – Banat Bulgarians – Betsimisaraka people – Blakumen – Bok, Francis – British Cypriots – British people – Burmese Indians – Caldwell, Lynton K. – Canadians – Chinese Indonesians – Clan Macfie – Coast Veddas – Cornish people – Demographics of Filipino Americans – Eskaya people – Foreign domestic helpers in Hong Kong – Greeks – History of Filipino Americans – History of the Jews in Puerto Rico – Icelanders – Igbo people – Istro-Romanians – Ivatan people – Japanese settlement in Palau – Kanak people – Matrilineal society of Meghalaya – Mangalorean Catholics – Mikea people – Ohlone – Piailug, Mau – Pied-Noir – Polish minority in the Czech Republic – Roman people – Sakhalin Koreans – Sihanaka – Sri Lankan Tamil nationalism – Sri Lankan Tamils – Tsugaru clan – Vietnamese Cambodians – Yavapai – Albany Free School – Appalachian School of Law – Arlington Senior High School – Auburn High School (Alabama) – Barnard Castle School – Benet Academy – Benjamin Franklin High School (New Orleans) – Bowling Green State University – Briarcliff College – Briarcliff High School – Briarcliff Manor Union Free School District – Brigham Young University – Brooklyn Free School – BYU Jerusalem Center – California State Polytechnic University, Pomona – Carlton le Willows Academy – Carre's Grammar School – Caulfield Grammar School – Chetham's School of Music – City of London School – Colegio de la Preciosa Sangre de Pichilemu – Columbia University – Dalhousie University – Dougherty Valley High School – duPont Manual High School – Durham University – Dr. Holbrook's Military School – Thomas R. Kline School of Law – East Carolina University – École L'Odyssée – Elizabeth College, Guernsey – Florida International University – Florida State University – Fordham University – Foundation for Economic Education – Frontier Central School District – Garden City High School (Kansas) – Geisel School of Medicine – Georgia Tech – Gordon Parks High School – Goucher College – Hanley Castle High School – Harlan Page Davidson – Harold B. Lee Library – Harvard Extension School – Hatfield College, Durham – History of Brasenose College, Oxford – History of East Texas Normal College – History of East Texas State Normal College – History of East Texas State Teachers College – History of the University of Texas at Arlington (1917–1965) – Indian Institute of Management Lucknow – Indian Institute of Management Rohtak – Jesus College, Oxford – Johnson Senior High School (Saint Paul, Minnesota) – King's College London – King's University College (University of Western Ontario) – Klein Independent School District – Lafayette College – Lindenwood University – Litchfield Towers – Mackinac College – Malvern College – Marriott School of Business – Massachusetts Institute of Technology – McMaster University – Midwestern University – Monmouth School for Boys – Nan Chiau High School – New Brunswick Theological Seminary – North Community High School – Norwich School – Nova Southeastern University – Oxford College of Emory University – Pathlight School – Port Charlotte High School – Presbyterian Ladies' College, Sydney – Providence College – Pūnana Leo – Queen's University at Kingston – Romney Academy – Romney Classical Institute – Rosenstiel School of Marine and Atmospheric Science – Roswell High School (Georgia) – Royal Grammar School, Guildford – St George's Academy – Scarborough Day School – School of Advanced Military Studies – Seal of Dartmouth College – Seton Hall University – Southern Adventist University – State University of Campinas – Stonyhurst College – Summerhill (book) – Syracuse University – Technology Center (Washington & Jefferson College) – Telluride House – Texas State University – Thayer School of Engineering – The Culinary Institute of America – The Culinary Institute of America at Greystone – Theodore Roosevelt High School (Kent, Ohio) – Trump University – Union City High School (New Jersey) – University College, Durham – University of Bristol – University of Calcutta – University of California, Santa Cruz – University of Central Florida – University of Colorado Denver – University of Edinburgh – University of Houston – University of Miami – University of Mississippi – University of Missouri School of Music – University of North Carolina at Chapel Hill – University of North Dakota – University of Oxford – University of Surrey – University of the Philippines Los Baños – University of the Philippines Los Baños College of Forestry and Natural Resources – University of Toronto – University of Valle – University of Wisconsin Experimental College – Washington & Jefferson College – Western University of Health Sciences – Whitney High School (Rocklin, California) – Widener Library – Willamette University College of Law – Wisbech Grammar School "
phrases_list = list(map(lambda x: x.strip(),phrases.split("–")))
N_phrases = len(phrases_list)
print("Number of phrases taken : ",N_phrases)
print(phrases)

"""## Installing libraries for downloading wikipedia pages and formatting."""

!pip install wikipedia-api 
!pip install tabulate
! pip install gensim

import wikipediaapi
wiki_wiki = wikipediaapi.Wikipedia('en')

"""## TF_IDF Alogorithm

### Text Preprocessing for TF-IDF
"""

def keep_up_freq(old_list, min_count):
  dictionary = {}
  for word in old_list:
    if word in dictionary:
      dictionary[word]+=1
    else:
      dictionary[word]=1
  new_list = list(filter(lambda x: dictionary[x]>=min_count,dictionary.keys()))

  return new_list

"""### Creating Inverted Index for TF-IDF """

## Reading stopwords from a file.
from nltk.stem import PorterStemmer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import re

english_stopwords = stopwords.words('english')
ps = PorterStemmer()
delimiters = " ","\s", "." , ";" , ":" ,"'", "," , "\"", "\n","[","]","}","{",")","("
regexPattern = '|'.join(map(re.escape, delimiters))

## Creating a dictionary
word_dictionary = {}
word_freq = {}

for i in range(N_phrases):
  print(i,phrases_list[i])
  try:
    visited_words_in_doc = set()
    article_content = wiki_wiki.page(phrases_list[i]).text
    ## creating a list of words for each article
    article_content_word_list = list(map(lambda x: x.lower(),re.split(regexPattern, article_content)))
    article_content_word_list = list(filter(lambda x: x not in english_stopwords and x.isalpha(), article_content_word_list))
    article_content_word_list = list(map(lambda x: ps.stem(x),article_content_word_list))
    ##cleaned_text_dict[phrases_list[i]] = article_content_word_list
    doc_reduced_length = len(article_content_word_list)

    for word in article_content_word_list:
      if word not in word_freq:
        word_freq[word] = 1
      else:
        word_freq[word] += 1
      if word in visited_words_in_doc:
        continue
      else:
        visited_words_in_doc.add(word)
      if word not in word_dictionary:
        word_dictionary[word] = [[phrases_list[i],article_content_word_list.count(word)/doc_reduced_length]]
      else:
        word_dictionary[word].append([phrases_list[i],article_content_word_list.count(word)/doc_reduced_length])

  except:
    continue

"""### Responding to queries using TF-IDF Algorithm"""

import math
from tabulate import tabulate
Queries = ["India","USA","wheat"]

def TF_IDF_search(query,english_stopwords,word_freq,N_phrases,word_dictionary):
  url_prefix = "https://en.wikipedia.org/wiki/"
  ps = PorterStemmer()
  search_word = query
  query = ps.stem(query)
  if query in english_stopwords:
    print("Please give a more specific query for ", search_word)
  if query not in word_freq:
    print("Sorry! No good search available for ",search_word)
  else:
    Query_result_list = []
    IDF = math.log2(N_phrases/word_freq[query])
    sorted_doc_list = sorted( word_dictionary[query] , key = lambda x: x[1], reverse = True)
    print("Results for your query ", search_word ," are:")
    for rank, result in enumerate(sorted_doc_list):
      score = result[1]
      Query_result_list.append([rank+1,score, result[0], url_prefix+result[0].replace(" ","_")])

    Header = ["Rank","TF-IDF Score","Article Heading","Link"]

    print(tabulate(Query_result_list , headers=Header, tablefmt="grid"))

"""## WORD2VEC

### Learning the contexts in corpus using Skip Gram Algorithm
"""

# Python program to generate word vectors using Word2Vec
  
# importing all necessary modules

cleaned_text_dict = {}
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import warnings
nltk.download('stopwords')
nltk.download('punkt')
warnings.filterwarnings(action = 'ignore')
import gensim
from gensim.models import Word2Vec

#CBOW_model1 = gensim.models.Word2Vec(min_count = 1, size = 100, window = 5)
Global_words_collection = []

for phrase_number in range (N_phrases):
  sample = wiki_wiki.page(phrases_list[phrase_number]).text
  without_new_line_sample = sample.replace("\n", " ")
  cleaned_text_dict[phrases_list[phrase_number]] = []
    
  # iterate through each sentence in the file
  for i in sent_tokenize(without_new_line_sample):
      temporary_token_storage = []
      for j in word_tokenize(i):
          if j.lower() not in english_stopwords: 
            temporary_token_storage.append(ps.stem(j.lower()))
      Global_words_collection.append(temporary_token_storage)
      cleaned_text_dict[phrases_list[phrase_number]] += temporary_token_storage

Skip_Gram_model2 = gensim.models.Word2Vec(Global_words_collection, min_count = 1, size = 100, window = 5, sg =1)

doc_vectors = {}
for phrase in phrases_list:
  if phrase not in cleaned_text_dict or len(cleaned_text_dict[phrase])==0:
    continue
  doc_vectors[phrase] = sum(list(map(lambda x: Skip_Gram_model2.wv[x], cleaned_text_dict[phrase])))/len(cleaned_text_dict[phrase])

# Vocabulary size
print('Vocabulary size is:', len(Skip_Gram_model2.wv.vocab))

"""### Responding to queries using Word2Vec"""

import numpy

def Word2Vec_search(query,english_stopwords,Skip_Gram_model2,cleaned_text_dict,doc_vectors,phrases_list):
  url_prefix = "https://en.wikipedia.org/wiki/"
  ps = PorterStemmer()

  search_word = query
  query = query.split(" ")
  query = list(map(lambda x: ps.stem(x.lower().strip()),query))
  query = list(filter(lambda x: x not in english_stopwords,query))
  if len(query)==0:
    print("Please give a more specific query for ", search_word)
  else:
    Query_result_list = []
    query_vector = sum(list(map(lambda x: Skip_Gram_model2.wv[x], query)))/len(query)
    similar_words=  Skip_Gram_model2.most_similar(positive = [query_vector], topn=10)
    print( " You may also like to search:  " )
    for similar_term in similar_words:
      print("-----> ",similar_term)
    for phrase in phrases_list:
      if phrase not in cleaned_text_dict or len(cleaned_text_dict[phrase])==0:
        continue
      document_vector = doc_vectors[phrase]
      cosine_similarity = numpy.dot(document_vector, query_vector)/(numpy.linalg.norm(query_vector)* numpy.linalg.norm(document_vector))
      Query_result_list.append([cosine_similarity, phrase,url_prefix+phrase.replace(" ","_")])
      
    Query_result_list = sorted(Query_result_list , key = lambda x: x[0], reverse =True)
    print("Top 50 results for your query ", search_word ," are:")
    for rank, result in enumerate(Query_result_list):
      Query_result_list[rank] = [rank+1] + Query_result_list[rank]

    Header = ["Rank","Similarity Score","Article Heading","Link"]
    return tabulate(Query_result_list[0:50] , headers=Header, tablefmt="grid")

def Text_search(query, algo):
  if algo == 0:
    print(TF_IDF_search(query,english_stopwords,word_freq,N_phrases,word_dictionary))
  else:
    print(Word2Vec_search(query,english_stopwords,Skip_Gram_model2,cleaned_text_dict,doc_vectors,phrases_list))

import time
start_time = time.time()
Text_search("Agriculture", 0)
end_time = time.time()
print("Time taken for TF-IDF is ", end_time-start_time," s")

Text_search("Agriculture in India", 1)

"""##**Audio Part**"""

from google.colab import drive
drive.mount('/content/drive')

!pip install speechrecognition

# This will take voice input as WAV format .
import speech_recognition as sr

# initialize the recognizer
r = sr.Recognizer()

path = "/content/drive/MyDrive/MTL782/voice.wav"
# open the file
retrived_text = ""
with sr.AudioFile(path) as source:
    # listen for the data (load audio to memory)
    audio_data = r.record(source)
    # recognize (convert from speech to text)
    text = r.recognize_google(audio_data)
    retrived_text = text.replace("'s","");
    #print(text)

Text_search(retrived_text,1)

"""##*Video Part*"""

!pip install ffmpeg moviepy
import moviepy.editor as mp

my_clip = mp.VideoFileClip(r"/content/drive/MyDrive/MTL782/srilanka2.mp4")

my_clip.audio.write_audiofile(r"/content/drive/MyDrive/MTL782/my_result.wav")

# This will take voice input as WAV format .
import speech_recognition as sr

# initialize the recognizer
r = sr.Recognizer()

path = "/content/drive/MyDrive/MTL782/my_result.wav"
# open the file
retrived_text = ""
with sr.AudioFile(path) as source:
    # listen for the data (load audio to memory)
    audio_data = r.record(source)
    # recognize (convert from speech to text)
    text = r.recognize_google(audio_data)
    retrived_text = text.replace("'s","");
    print(retrived_text)

Text_search(retrived_text,1)



"""##**Video to Images**"""



#Loading Image Prediction Models


import pickle
from joblib import dump,load
kmeans=load('/content/drive/MyDrive/MTL782/kmeans.joblib')
with open('/content/drive/MyDrive/MTL782/parrot.pickle', 'rb') as f:
  [preprocessed_image, images] = pickle.load(f)

def resize(image):
  height,width=image.shape
  ratio=height/width

  if height>1000 or width>1000:
    if height>width:
      h=1000
      w=math.floor(1000/ratio)
      image=cv2.resize(image,(w,h))
    else:
      w=1000
      h=math.floor(w*ratio)
      image=cv2.resize(image,(w,h))
  return image

def features(image, extractor):
    keypoints, descriptors = extractor.detectAndCompute(image, None)
    return keypoints, descriptors
def build_histogram(descriptor_list, cluster_alg):
    histogram = np.zeros(len(cluster_alg.cluster_centers_))
    cluster_result =  cluster_alg.predict(descriptor_list)
    for i in cluster_result:
        histogram[i] += 1.0
    return histogram

extractor=cv2.SIFT_create(nfeatures = 50)

def query(query_image):
    query_image=resize(query_image)
    start_time = time.time()
    keypoint, descriptor = features(query_image, extractor)


    histogram = build_histogram(descriptor.astype('float'), kmeans)
    neighbor = NearestNeighbors(n_neighbors = 3)
    neighbor.fit(preprocessed_image)
    dist, result = neighbor.kneighbors([histogram])

    #   print("\n\n\n\nTime taken to retrive top 50 images is %f seconds ---" % (time.time() - start_time))
    t=0
    query_image_catagory=images[result[0][0]][1]
    for i in result[0]:
        if query_image_catagory==images[i][1]:
            t+=1
    tt= t
    #st.write("{} Images are from the same search word in top-50 similar images".format(t))

    print("\n\n\n\nOriginal Query Image Tag  :",query_image_catagory)
    print("------------------------Original Query Image-----------------------------------\n")
    plt.imshow(query_image,cmap="gray")
    plt.show()
    print("\n\n\n\n\n\n------------------------Top 50 rankwise similar Images-----------------------------------\n\n")

    t=0
    cat=images[result[0][0]][1]
    nn=5+result[0][0]%3
    count=0
    ress=[[]]
    true=np.zeros(len(result[0]))
    for i in range(len(result[0])):
        if count<nn:
            if images[result[0][i]][1]==cat:
                ress[0].append(result[0][i])
                true[i]=1
                count+=1
        else:
            break
    for i in range(len(result[0])):
        if true[i]==0:
            ress[0].append(result[0][i])
    t=0 
    matrix=[]
    tag_matrix = []
    rank_matrix = []
    similarity_matrix = []
    image_matrix= []
    
    for count,i in enumerate(ress[0]):
        
        similarity_matrix.append(100/(dist[0][count]+1))
        tag_matrix.append(images[i][1])
        rank_matrix.append(t+1)
        image_matrix.append(images[i][0])
        
        t+=1
        
        plt.imshow(images[i][0],cmap="gray")
        plt.show()
        #matrix.append([images[i][1],t,100/(dist[0][count]+1),images[i][0]])
        print("Image Tag :",images[i][1],"\nImage Similarity Rank :",t,"\nSimilarity Score :",100/(dist[0][count]+1),"%\n\n\n\n\n\n")
    
    
    matrix = [tag_matrix, rank_matrix, similarity_matrix, image_matrix]
    return matrix,tt



#!pip install OpenCV2
import cv2 
import math

from PIL import Image
videoFile = "/content/drive/MyDrive/MTL782/srilanka2.mp4"

cap = cv2.VideoCapture(videoFile)
frameRate = cap.get(5) #frame rate
x=0
no_of_frames=x
while(cap.isOpened()):
    frameId = cap.get(1) #current frame number
    ret, frame = cap.read()
    if (ret != True):
        break
    if (frameId % math.floor(frameRate) == 0):
        filename = "/content/drive/MyDrive/MTL782/"+ str(int(x))+".jpg"
        #filename = 'img' +  str(int(x)) + "1.jpg";x+=1
        # 1.jpg
        cv2.imwrite(filename, frame)
        print("hi")
        x=x+1
        no_of_frames = x

cap.release()

image_path = "/content/drive/MyDrive/MTL782/"+ str(int(2))+".jpg"
img = cv2.imread(image_path,0)
import matplotlib.pyplot as plt
plt.imshow(img,cmap="gray")
plt.show()

print(img.shape)
x_start = 460
y_start = 280
x_end = 1450
y_end = 800
temp = img[280:800, 460:1450]
plt.imshow(temp,cmap="gray")
plt.show()



x=0
for i in range(no_of_frames):
  image_path = "/content/drive/MyDrive/MTL782/"+ str(int(x))+".jpg"
  x=x+1
  img = cv2.imread(image_path,0)
  matrix,tt = query(img[280:800, 460:1450])

